{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('train2.csv')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"The dataset file 'your_dataset.csv' was not found. Please ensure the file exists and the path is correct.\")\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = ['extracted_text', 'entity_name', 'entity_value']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing from the dataset: {', '.join(missing_columns)}\")\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df_clean = df.dropna(subset=required_columns)\n",
    "if len(df_clean) < len(df):\n",
    "    print(f\"Removed {len(df) - len(df_clean)} rows with NaN values.\")\n",
    "    df = df_clean\n",
    "\n",
    "# Combine extracted_text and entity_name\n",
    "df['input_text'] = df['extracted_text'].astype(str) + ' [SEP] ' + df['entity_name'].astype(str)\n",
    "\n",
    "# Create a dictionary to map entity_value to integer labels\n",
    "unique_values = df['entity_value'].unique()\n",
    "value_to_label = {value: idx for idx, value in enumerate(unique_values)}\n",
    "label_to_value = {idx: value for value, idx in value_to_label.items()}\n",
    "\n",
    "# Convert entity_value to integer labels\n",
    "df['label'] = df['entity_value'].map(value_to_label)\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['input_text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_values))\n",
    "\n",
    "# Function to safely tokenize texts\n",
    "def safe_tokenize(texts, max_length=128):\n",
    "    try:\n",
    "        return tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {str(e)}\")\n",
    "        print(\"Problematic texts:\")\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str):\n",
    "                print(f\"  - {text} (type: {type(text)})\")\n",
    "        raise\n",
    "\n",
    "# Tokenize and encode the texts\n",
    "try:\n",
    "    train_encodings = safe_tokenize(train_texts)\n",
    "    val_encodings = safe_tokenize(val_texts)\n",
    "except Exception as e:\n",
    "    print(f\"Tokenization failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels)\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(val_true, val_preds, average='weighted')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation F1 Score: {f1:.4f}')\n",
    "\n",
    "# Function to predict entity_value\n",
    "def predict_entity_value(text, entity_name):\n",
    "    input_text = str(text) + ' [SEP] ' + str(entity_name)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return label_to_value[pred]\n",
    "\n",
    "# Example usage\n",
    "example_text = \"Product weight: 500g\"\n",
    "example_entity_name = \"item_weight\"\n",
    "predicted_value = predict_entity_value(example_text, example_entity_name)\n",
    "print(f\"Predicted entity value: {predicted_value}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('entity_value_predictor_model')\n",
    "tokenizer.save_pretrained('entity_value_predictor_tokenizer')\n",
    "\n",
    "# To load the model later:\n",
    "# loaded_model = BertForSequenceClassification.from_pretrained('entity_value_predictor_model')\n",
    "# loaded_tokenizer = BertTokenizer.from_pretrained('entity_value_predictor_tokenizer')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
