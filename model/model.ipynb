{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define base directory for all saved files\n",
    "BASE_DIR = '/kaggle/working/'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/image-value/merged_output_f3.csv')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"The dataset file 'your_dataset.csv' was not found. Please ensure the file exists and the path is correct.\")\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = ['extracted_text', 'entity_name', 'entity_value']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing from the dataset: {', '.join(missing_columns)}\")\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df_clean = df.dropna(subset=required_columns)\n",
    "if len(df_clean) < len(df):\n",
    "    print(f\"Removed {len(df) - len(df_clean)} rows with NaN values.\")\n",
    "    df = df_clean\n",
    "\n",
    "# Combine extracted_text and entity_name\n",
    "df['input_text'] = df['extracted_text'].astype(str) + ' [SEP] ' + df['entity_name'].astype(str)\n",
    "\n",
    "# Create a dictionary to map entity_value to integer labels\n",
    "unique_values = df['entity_value'].unique()\n",
    "value_to_label = {value: idx for idx, value in enumerate(unique_values)}\n",
    "label_to_value = {idx: value for value, idx in value_to_label.items()}\n",
    "\n",
    "# Convert entity_value to integer labels\n",
    "df['label'] = df['entity_value'].map(value_to_label)\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['input_text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_values))\n",
    "\n",
    "# Function to safely tokenize texts\n",
    "def safe_tokenize(texts, max_length=128):\n",
    "    try:\n",
    "        return tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {str(e)}\")\n",
    "        print(\"Problematic texts:\")\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str):\n",
    "                print(f\"  - {text} (type: {type(text)})\")\n",
    "        raise\n",
    "\n",
    "# Tokenize and encode the texts\n",
    "try:\n",
    "    train_encodings = safe_tokenize(train_texts)\n",
    "    val_encodings = safe_tokenize(val_texts)\n",
    "except Exception as e:\n",
    "    print(f\"Tokenization failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels)\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Checkpoint functions\n",
    "def save_checkpoint(epoch, model, optimizer, best_f1):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint_dir = os.path.join(BASE_DIR, 'checkpoints')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_f1': best_f1,\n",
    "        'value_to_label': value_to_label,\n",
    "        'label_to_value': label_to_value\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "    \"\"\"Load latest checkpoint if it exists\"\"\"\n",
    "    checkpoint_dir = os.path.join('/kaggle/input', 'checkpoint')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return 0, 0.0  # Return default values if no checkpoint exists\n",
    "    \n",
    "    # Find the latest checkpoint\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_')]\n",
    "    if not checkpoints:\n",
    "        print(\"heloooooo\")\n",
    "        return 0, 0.0\n",
    "    \n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        global value_to_label, label_to_value\n",
    "        value_to_label = checkpoint['value_to_label']\n",
    "        label_to_value = checkpoint['label_to_value']\n",
    "        \n",
    "        print(f\"Resumed from epoch {checkpoint['epoch']} with best F1 score: {checkpoint['best_f1']:.4f}\")\n",
    "        return checkpoint['epoch'], checkpoint['best_f1']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {str(e)}\")\n",
    "        return 0, 0.0\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch, best_f1 = load_checkpoint(model, optimizer)\n",
    "print(\"vasuuuuuu\")\n",
    "num_epochs = 25\n",
    "for epoch in range (num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    current_f1 = f1_score(val_true, val_preds, average='weighted')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation F1 Score: {current_f1:.4f}')\n",
    "    \n",
    "    # Save checkpoint if we have the best score\n",
    "    if current_f1 > best_f1:\n",
    "        best_f1 = current_f1\n",
    "    if((epoch+1)%5==0):\n",
    "        save_checkpoint(epoch + 1, model, optimizer, best_f1)\n",
    "\n",
    "# Function to predict entity_value\n",
    "def predict_entity_value(text, entity_name):\n",
    "    input_text = str(text) + ' [SEP] ' + str(entity_name)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return label_to_value[pred]\n",
    "\n",
    "# Example usage\n",
    "example_text = \"Product weight: 500g\"\n",
    "example_entity_name = \"item_weight\"\n",
    "predicted_value = predict_entity_value(example_text, example_entity_name)\n",
    "print(f\"Predicted entity value: {predicted_value}\")\n",
    "\n",
    "# Save the model and tokenizer in the Kaggle working directory\n",
    "model_save_path = os.path.join(BASE_DIR, 'entity_value_predictor_model')\n",
    "tokenizer_save_path = os.path.join(BASE_DIR, 'entity_value_predictor_tokenizer')\n",
    "mappings_save_path = os.path.join(BASE_DIR, 'entity_value_predictor_mappings.json')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "os.makedirs(tokenizer_save_path, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "# Save the label mappings\n",
    "mappings = {\n",
    "    'value_to_label': value_to_label,\n",
    "    'label_to_value': label_to_value\n",
    "}\n",
    "with open(mappings_save_path, 'w') as f:\n",
    "    json.dump(mappings, f)\n",
    "\n",
    "print(f\"\\nModel saved to: {model_save_path}\")\n",
    "print(f\"Tokenizer saved to: {tokenizer_save_path}\")\n",
    "print(f\"Mappings saved to: {mappings_save_path}\")\n",
    "\n",
    "# To load the model later:\n",
    "# loaded_model = BertForSequenceClassification.from_pretrained('/kaggle/working/entity_value_predictor_model')\n",
    "# loaded_tokenizer = BertTokenizer.from_pretrained('/kaggle/working/entity_value_predictor_tokenizer')\n",
    "# with open('/kaggle/working/entity_value_predictor_mappings.json', 'r') as f:\n",
    "#     loaded_mappings = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
